{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Configuração Inicial"
   ],
   "metadata": {
    "collapsed": false,
    "id": "gTwmW32LdbDo"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Configuração e import de bibliotecas."
   ],
   "metadata": {
    "collapsed": false,
    "id": "5tpyf1WZdbDp"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from os import getcwd\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from funcoes_locais import reajustar_imagem\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "diretorio_treino = 'asl-alphabet/asl_alphabet_train/asl_alphabet_train/'\n",
    "diretorio_teste = 'asl-alphabet/asl_alphabet_test/'\n",
    "diretorio_atual = getcwd()"
   ],
   "metadata": {
    "id": "Ct74BCICdbDp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importação dos dados - via diretório\n",
    "\n",
    "Essa é a forma ideal de importar os dados, já que ela permite a separação das bases de treino e validação no momento da importação.\n",
    "Porém, precisamos pegar apenas uma parte das bases de treino e validação para fins de velocidade do treino.\n",
    "Seguindo o downloado e redimensionamento feito na parte de pré-processamento, cada classe terá 150 amostras."
   ],
   "metadata": {
    "id": "CeWSVpgaKqj4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Agora, será montado o dataset de treino, validação e teste, usando keras."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset = keras.utils.image_dataset_from_directory(\n",
    "    diretorio_treino,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=32,\n",
    "    image_size=(200, 200),\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validation_split=0.25,\n",
    "    subset='training',\n",
    "    interpolation='bilinear',\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False\n",
    ")\n",
    "\n",
    "\n",
    "val_dataset = keras.utils.image_dataset_from_directory(\n",
    "    diretorio_treino,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=32,\n",
    "    image_size=(200, 200),\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validation_split=0.25,\n",
    "    subset='validation',\n",
    "    interpolation='bilinear',\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False\n",
    ")\n",
    "\n",
    "\n",
    "test_dataset = keras.utils.image_dataset_from_directory(\n",
    "    diretorio_teste,\n",
    "    labels=list(range(0,28)),\n",
    "    label_mode='int',\n",
    "    class_names=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=32,\n",
    "    image_size=(200, 200),\n",
    "    shuffle=False,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation='bilinear',\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ArWkm5P29Mm3",
    "outputId": "d7316100-07dc-4674-e0f3-202ddea23278"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tendo as imagens carregadas, vamos aplicar a função de pré-processamento, para assim podermos alimentar o modelo:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#train_dataset = train_dataset.map(lambda x, y: reajustar_imagem(x, y))\n",
    "#val_dataset = val_dataset.map(lambda x, y: reajustar_imagem(x, y))\n",
    "#test_dataset = test_dataset.map(lambda x, y: reajustar_imagem(x, y))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#  Configuração de Pipeline\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ],
   "metadata": {
    "id": "8z6uS0mHT_Mn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "num_classes = 29\n",
    "\n",
    "pretrained_base = tf.keras.applications.inception_v3.InceptionV3(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_tensor=None,\n",
    "    input_shape=(200, 200, 3),\n",
    "    pooling='max',\n",
    "    classes=num_classes,\n",
    "    classifier_activation='softmax'\n",
    ")\n",
    "\n",
    "pretrained_base.Trainable = False\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    pretrained_base,\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.3),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    min_delta=0.00001,\n",
    "    monitor='val_loss',\n",
    "    patience=16,\n",
    "    restore_best_weights=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.7,\n",
    "                              patience=5,\n",
    "                              min_lr=0.0001)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.007),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 6\n",
    "\n",
    "history = model.fit(\n",
    "  train_dataset,\n",
    "  validation_data = val_dataset,\n",
    "  batch_size=32,\n",
    "  epochs=epochs,\n",
    "  callbacks=[early_stopping, reduce_lr]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save(\"mymodel.keras\")\n",
    "modelo = keras.models.load_model(\"mymodel.keras\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 296ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       1.00      1.00      1.00         1\n",
      "           2       1.00      1.00      1.00         1\n",
      "           3       1.00      1.00      1.00         1\n",
      "           4       1.00      1.00      1.00         1\n",
      "           5       1.00      1.00      1.00         1\n",
      "           6       1.00      1.00      1.00         1\n",
      "           7       1.00      1.00      1.00         1\n",
      "           8       1.00      1.00      1.00         1\n",
      "           9       1.00      1.00      1.00         1\n",
      "          10       1.00      1.00      1.00         1\n",
      "          11       1.00      1.00      1.00         1\n",
      "          12       1.00      1.00      1.00         1\n",
      "          13       1.00      1.00      1.00         1\n",
      "          14       1.00      1.00      1.00         1\n",
      "          15       1.00      1.00      1.00         1\n",
      "          16       1.00      1.00      1.00         1\n",
      "          17       1.00      1.00      1.00         1\n",
      "          18       1.00      1.00      1.00         1\n",
      "          19       1.00      1.00      1.00         1\n",
      "          20       1.00      1.00      1.00         1\n",
      "          21       1.00      1.00      1.00         1\n",
      "          22       1.00      1.00      1.00         1\n",
      "          23       1.00      1.00      1.00         1\n",
      "          24       1.00      1.00      1.00         1\n",
      "          25       1.00      1.00      1.00         1\n",
      "          26       0.00      0.00      0.00         1\n",
      "          27       0.00      0.00      0.00         1\n",
      "          28       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.93        28\n",
      "   macro avg       0.90      0.90      0.90        28\n",
      "weighted avg       0.93      0.93      0.93        28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kafat\\anaconda3\\envs\\tf-directml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Kafat\\anaconda3\\envs\\tf-directml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Kafat\\anaconda3\\envs\\tf-directml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Kafat\\anaconda3\\envs\\tf-directml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Kafat\\anaconda3\\envs\\tf-directml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Kafat\\anaconda3\\envs\\tf-directml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_dataset, verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = []\n",
    "for _, labels in test_dataset:\n",
    "    true_classes.extend(labels.numpy())\n",
    "\n",
    "confusion_mtx = confusion_matrix(true_classes, predicted_classes)\n",
    "class_names = [str(i) for i in range(0,29)]\n",
    "classification_rep = classification_report(true_classes, predicted_classes, target_names=class_names)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_rep)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T20:17:00.261310200Z",
     "start_time": "2023-10-03T20:16:59.870272Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = model.predict(test_dataset, verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "predicted_classes"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
